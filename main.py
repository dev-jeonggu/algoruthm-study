import os
import re
import sys
import requests
import certifi
from bs4 import BeautifulSoup, Tag
from datetime import datetime

def extract_metadata(java_file_path):
    """Java íŒŒì¼ì—ì„œ ë©”íƒ€ë°ì´í„° ì¶”ì¶œ"""
    with open(java_file_path, 'r', encoding='utf-8') as f:
        lines = f.readlines()

    title = platform = url = None
    for line in lines:
        if "NOTE" in line:
            content = line.strip().split(":", 1)[-1].strip()
            if content.startswith("http"):
                url = content
            elif content.lower() in ["í”„ë¡œê·¸ë˜ë¨¸ìŠ¤", "ë°±ì¤€"]:
                platform = content.lower()
            else:
                title = content.strip()

    return title, platform, url

def slugify(text):
    """í…ìŠ¤íŠ¸ë¥¼ íŒŒì¼ëª…ìœ¼ë¡œ ì‚¬ìš© ê°€ëŠ¥í•œ í˜•íƒœë¡œ ë³€í™˜"""
    return text.strip().replace(" ", "_").replace(":", "").replace("/", "_")

def extract_programmers_markdown(soup):
    """í”„ë¡œê·¸ë˜ë¨¸ìŠ¤ í˜ì´ì§€ì—ì„œ ë§ˆí¬ë‹¤ìš´ ì¶”ì¶œ"""
    content = {
        "ë¬¸ì œ ì„¤ëª…": "",
        "ì œí•œì‚¬í•­": "",
        "ì…ì¶œë ¥ ì˜ˆ": "",
        "ì…ì¶œë ¥ ì˜ˆ ì„¤ëª…": ""
    }
    
    # ì—¬ëŸ¬ ê°€ëŠ¥í•œ ì„ íƒì ì‹œë„
    desc_selectors = [
        ".markdown",
        ".lesson-content",
        ".problem-description", 
        "[class*='markdown']",
        "[class*='content']"
    ]
    
    desc_box = None
    for selector in desc_selectors:
        desc_box = soup.select_one(selector)
        if desc_box:
            print(f"âœ“ ì„¤ëª… ì˜ì—­ ì°¾ìŒ: {selector}")
            break
    
    if not desc_box:
        # ëŒ€ì•ˆ: ì „ì²´ í…ìŠ¤íŠ¸ì—ì„œ íŒ¨í„´ìœ¼ë¡œ ì°¾ê¸°
        print("âš ï¸ .markdown ì˜ì—­ì„ ì°¾ì„ ìˆ˜ ì—†ì–´ ì „ì²´ í…ìŠ¤íŠ¸ íŒŒì‹± ì‹œë„")
        full_text = soup.get_text()
        
        # í…ìŠ¤íŠ¸ì—ì„œ ê° ì„¹ì…˜ ì¶”ì¶œ
        sections = re.split(r'\n\s*(ë¬¸ì œ ì„¤ëª…|ì œí•œì‚¬í•­|ì…ì¶œë ¥ ì˜ˆ|ì…ì¶œë ¥ ì˜ˆ ì„¤ëª…)\s*\n', full_text)
        
        current_section = None
        for i, section in enumerate(sections):
            section_title = section.strip()
            if section_title in content:
                current_section = section_title
            elif current_section and section.strip():
                # ë‹¤ìŒ ì„¹ì…˜ì´ ë‚˜ì˜¤ê¸° ì „ê¹Œì§€ì˜ ë‚´ìš©ì„ ìˆ˜ì§‘
                content_text = section.strip()
                # ë„ˆë¬´ ê¸´ ë‚´ìš©ì€ ì ë‹¹íˆ ìë¥´ê¸°
                if len(content_text) > 2000:
                    content_text = content_text[:2000] + "..."
                content[current_section] = content_text
                current_section = None
        
        return content

    current = "ë¬¸ì œ ì„¤ëª…"
    buffer = []

    # ëª¨ë“  ìì‹ ìš”ì†Œ ì²˜ë¦¬
    for element in desc_box.descendants:
        if hasattr(element, 'name'):
            if element.name == 'h5':
                # ì´ì „ ë²„í¼ ë‚´ìš© ì €ì¥
                if buffer:
                    content[current] = "\n".join(buffer).strip()
                    buffer = []
                
                heading = element.get_text(strip=True)
                print(f"ğŸ” ë°œê²¬ëœ í—¤ë”©: '{heading}'")
                
                # í—¤ë”©ì´ ìš°ë¦¬ê°€ ì°¾ëŠ” ì„¹ì…˜ ì¤‘ í•˜ë‚˜ì¸ì§€ í™•ì¸
                if heading in content:
                    current = heading
                elif "ë¬¸ì œ" in heading:
                    current = "ë¬¸ì œ ì„¤ëª…"
                elif "ì œí•œ" in heading:
                    current = "ì œí•œì‚¬í•­"
                elif "ì…ì¶œë ¥" in heading and "ì˜ˆ" in heading and "ì„¤ëª…" not in heading:
                    current = "ì…ì¶œë ¥ ì˜ˆ"
                elif "ì…ì¶œë ¥" in heading and "ì„¤ëª…" in heading:
                    current = "ì…ì¶œë ¥ ì˜ˆ ì„¤ëª…"
                    
            elif element.name == 'p':
                text = element.get_text(strip=True)
                if text:
                    buffer.append(text)
                    
            elif element.name == 'ul':
                for li in element.find_all('li'):
                    buffer.append(f"- {li.get_text(strip=True)}")
                    
            elif element.name == 'table':
                rows = element.find_all('tr')
                table_lines = []
                for i, row in enumerate(rows):
                    cols = [td.get_text(strip=True) for td in row.find_all(['td', 'th'])]
                    line = " | ".join(cols)
                    table_lines.append(line)
                    if i == 0:
                        table_lines.append(" | ".join(['---'] * len(cols)))
                buffer.extend(table_lines)
                
            elif element.name == 'br':
                buffer.append("")

    if buffer:
        content[current] = "\n".join(buffer).strip()

    return content
    
def load_snapshot(url):
    safe = re.sub(r'[^a-zA-Z0-9]', '_', url)
    path = f"snapshots/{safe}.html"

    # 1ï¸âƒ£ snapshotì´ ì´ë¯¸ ìˆìœ¼ë©´ ê·¸ëŒ€ë¡œ ì‚¬ìš©
    if os.path.exists(path):
        print(f"âœ“ Snapshot loaded: {path}")
        with open(path, encoding="utf-8") as f:
            return BeautifulSoup(f, "html.parser")

    # 2ï¸âƒ£ GitHub Actionsì—ì„œëŠ” ì ˆëŒ€ requests ê¸ˆì§€
    if os.getenv("GITHUB_ACTIONS") == "true":
        raise Exception(f"âŒ Snapshot not found in CI: {path}")

    # 3ï¸âƒ£ ë¡œì»¬ì—ì„œë§Œ fallback ë‹¤ìš´ë¡œë“œ
    print(f"ğŸ“¥ Local fallback download: {url}")
    headers = {
        "User-Agent": (
            "Mozilla/5.0 (Windows NT 10.0; Win64; x64) "
            "AppleWebKit/537.36 (KHTML, like Gecko) "
            "Chrome/91.0.4472.124 Safari/537.36"
        ),
        "Accept-Language": "ko-KR,ko;q=0.9",
    }

    response = requests.get(url, headers=headers, timeout=30)
    response.raise_for_status()

    os.makedirs("snapshots", exist_ok=True)
    with open(path, "w", encoding="utf-8") as f:
        f.write(response.text)

    return BeautifulSoup(response.text, "html.parser")
        
def fetch_baekjoon_content(url):
    """ë°±ì¤€ í˜ì´ì§€ì—ì„œ ë¬¸ì œ ë‚´ìš© ì¶”ì¶œ - HTML êµ¬ì¡° ê¸°ë°˜ íŒŒì‹±"""
    try:
        headers = {
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'
        }
        response = requests.get(url, headers=headers, verify=certifi.where(), timeout=30)
        response.raise_for_status()
    except Exception as e:
        raise Exception(f"âŒ ë°±ì¤€ URL ì ‘ì† ì‹¤íŒ¨: {url}\n{str(e)}")

    soup = BeautifulSoup(response.text, 'html.parser')
    
    # ì´ˆê¸°í™”
    ë¬¸ì œ = ""
    ì…ë ¥ = ""
    ì¶œë ¥ = ""
    ì˜ˆì œì…ë ¥ = []
    ì˜ˆì œì¶œë ¥ = []
    
    # 1. ë¬¸ì œ ì˜ì—­ ì°¾ê¸° (id="problem_description")
    problem_section = soup.find('div', {'id': 'problem_description'})
    if problem_section:
        # ëª¨ë“  p íƒœê·¸ì˜ í…ìŠ¤íŠ¸ë¥¼ í•©ì¹˜ê¸°
        ë¬¸ì œ_paragraphs = problem_section.find_all('p')
        ë¬¸ì œ = '\n'.join([p.get_text(strip=True) for p in ë¬¸ì œ_paragraphs])
        print(f"âœ“ ë¬¸ì œ ì„¤ëª… ì°¾ìŒ: {len(ë¬¸ì œ)}ì")
    
    # 2. ì…ë ¥ ì˜ì—­ ì°¾ê¸° (id="problem_input")
    input_section = soup.find('div', {'id': 'problem_input'})
    if input_section:
        ì…ë ¥_paragraphs = input_section.find_all('p')
        ì…ë ¥ = '\n'.join([p.get_text(strip=True) for p in ì…ë ¥_paragraphs])
        print(f"âœ“ ì…ë ¥ ì„¤ëª… ì°¾ìŒ: {len(ì…ë ¥)}ì")
    
    # 3. ì¶œë ¥ ì˜ì—­ ì°¾ê¸° (id="problem_output")
    output_section = soup.find('div', {'id': 'problem_output'})
    if output_section:
        ì¶œë ¥_paragraphs = output_section.find_all('p')
        ì¶œë ¥ = '\n'.join([p.get_text(strip=True) for p in ì¶œë ¥_paragraphs])
        print(f"âœ“ ì¶œë ¥ ì„¤ëª… ì°¾ìŒ: {len(ì¶œë ¥)}ì")
    
    # 4. ì˜ˆì œ ì…ì¶œë ¥ ì°¾ê¸°
    # ì˜ˆì œ ì…ë ¥ ì°¾ê¸° (class="sampledata")
    sample_inputs = soup.find_all('pre', {'class': 'sampledata', 'id': lambda x: x and x.startswith('sample-input-')})
    for sample in sample_inputs:
        text = sample.get_text(strip=True)
        if text:
            ì˜ˆì œì…ë ¥.append(text)
            print(f"âœ“ ì˜ˆì œ ì…ë ¥ ì°¾ìŒ: {text}")
    
    # ì˜ˆì œ ì¶œë ¥ ì°¾ê¸°
    sample_outputs = soup.find_all('pre', {'class': 'sampledata', 'id': lambda x: x and x.startswith('sample-output-')})
    for sample in sample_outputs:
        text = sample.get_text(strip=True)
        if text:
            ì˜ˆì œì¶œë ¥.append(text)
            print(f"âœ“ ì˜ˆì œ ì¶œë ¥ ì°¾ìŒ: {text}")
    
    # ë§Œì•½ ìœ„ ë°©ë²•ìœ¼ë¡œ ëª» ì°¾ì•˜ìœ¼ë©´ ë‹¤ë¥¸ ë°©ë²• ì‹œë„
    if not ì˜ˆì œì…ë ¥ or not ì˜ˆì œì¶œë ¥:
        # ëª¨ë“  pre íƒœê·¸ í™•ì¸
        all_pre = soup.find_all('pre')
        for i, pre in enumerate(all_pre):
            text = pre.get_text(strip=True)
            # copy ë²„íŠ¼ì´ ìˆëŠ” pre íƒœê·¸ëŠ” ì˜ˆì œì¼ ê°€ëŠ¥ì„±ì´ ë†’ìŒ
            if pre.find_next_sibling('button') or pre.find_previous_sibling('button'):
                copy_button_text = str(pre.find_next_sibling()) + str(pre.find_previous_sibling())
                if 'ë³µì‚¬' in copy_button_text:
                    if i % 2 == 0:  # ì§ìˆ˜ ì¸ë±ìŠ¤ëŠ” ì…ë ¥
                        ì˜ˆì œì…ë ¥.append(text)
                    else:  # í™€ìˆ˜ ì¸ë±ìŠ¤ëŠ” ì¶œë ¥
                        ì˜ˆì œì¶œë ¥.append(text)
    
    # í…ìŠ¤íŠ¸ ê¸°ë°˜ ë°±ì—… ë°©ë²•
    if not ë¬¸ì œ or not ì…ë ¥ or not ì¶œë ¥:
        print("âš ï¸ HTML êµ¬ì¡°ë¡œ ëª» ì°¾ì•„ì„œ í…ìŠ¤íŠ¸ ê¸°ë°˜ íŒŒì‹± ì‹œë„")
        
        # ì „ì²´ í…ìŠ¤íŠ¸ ê°€ì ¸ì˜¤ê¸°
        full_text = soup.get_text()
        
        # ë¬¸ì œ, ì…ë ¥, ì¶œë ¥ ì„¹ì…˜ ì°¾ê¸°
        sections = re.split(r'\n(ë¬¸ì œ|ì…ë ¥|ì¶œë ¥|ì˜ˆì œ ì…ë ¥|ì˜ˆì œ ì¶œë ¥)', full_text)
        
        current_section = None
        for i, section in enumerate(sections):
            section = section.strip()
            if section == 'ë¬¸ì œ':
                current_section = 'ë¬¸ì œ'
            elif section == 'ì…ë ¥':
                current_section = 'ì…ë ¥'
            elif section == 'ì¶œë ¥':
                current_section = 'ì¶œë ¥'
            elif section == 'ì˜ˆì œ ì…ë ¥':
                current_section = 'ì˜ˆì œ ì…ë ¥'
            elif section == 'ì˜ˆì œ ì¶œë ¥':
                current_section = 'ì˜ˆì œ ì¶œë ¥'
            elif current_section and section:
                if current_section == 'ë¬¸ì œ' and not ë¬¸ì œ:
                    ë¬¸ì œ = section
                elif current_section == 'ì…ë ¥' and not ì…ë ¥:
                    ì…ë ¥ = section
                elif current_section == 'ì¶œë ¥' and not ì¶œë ¥:
                    ì¶œë ¥ = section
                elif current_section == 'ì˜ˆì œ ì…ë ¥' and not ì˜ˆì œì…ë ¥:
                    ì˜ˆì œì…ë ¥.append(section)
                elif current_section == 'ì˜ˆì œ ì¶œë ¥' and not ì˜ˆì œì¶œë ¥:
                    ì˜ˆì œì¶œë ¥.append(section)
    
    # ê²°ê³¼ ì •ë¦¬
    result = {
        "ë¬¸ì œ": ë¬¸ì œ.strip() if ë¬¸ì œ else "",
        "ì…ë ¥": ì…ë ¥.strip() if ì…ë ¥ else "",
        "ì¶œë ¥": ì¶œë ¥.strip() if ì¶œë ¥ else "",
        "ì˜ˆì œ ì…ë ¥": ì˜ˆì œì…ë ¥[0] if ì˜ˆì œì…ë ¥ else "",
        "ì˜ˆì œ ì¶œë ¥": ì˜ˆì œì¶œë ¥[0] if ì˜ˆì œì¶œë ¥ else ""
    }
    
    # ë””ë²„ê¹… ì •ë³´
    print("\nğŸ“‹ ìµœì¢… ì¶”ì¶œ ê²°ê³¼:")
    for key, value in result.items():
        if value:
            print(f"  {key}: {value[:50]}..." if len(value) > 50 else f"  {key}: {value}")
        else:
            print(f"  {key}: âŒ ì—†ìŒ")
    
    # í•„ìˆ˜ í•­ëª© í™•ì¸
    if not result["ë¬¸ì œ"]:
        print("\nâš ï¸ ë¬¸ì œ ì„¤ëª…ì„ ì°¾ì§€ ëª»í–ˆìŠµë‹ˆë‹¤. í˜ì´ì§€ êµ¬ì¡° í™•ì¸ í•„ìš”")
    if not result["ì˜ˆì œ ì…ë ¥"] or not result["ì˜ˆì œ ì¶œë ¥"]:
        print("\nâš ï¸ ì˜ˆì œë¥¼ ì°¾ì§€ ëª»í–ˆìŠµë‹ˆë‹¤. ìˆ˜ë™ìœ¼ë¡œ ì…ë ¥í•´ì•¼ í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.")
    
    return result

def create_markdown(platform, title, url, content):
    """ë§ˆí¬ë‹¤ìš´ íŒŒì¼ ìƒì„±"""
    if platform == "programmers":
        # í”„ë¡œê·¸ë˜ë¨¸ìŠ¤: problems/programmers/ë¬¸ì œëª…/ë¬¸ì œëª….md
        dir_name = slugify(title)
        base_path = f"problems/programmers/{dir_name}"
        md_filename = f"{dir_name}.md"
    else:
        # ë°±ì¤€: problems/baekjoon/ë¬¸ì œë²ˆí˜¸_ë¬¸ì œëª…/ë¬¸ì œëª….md (ë¬¸ì œë²ˆí˜¸ ì œì™¸)
        dir_name = slugify(title)  # 31995_ê²Œì„ë§ì˜¬ë ¤ë†“ê¸°
        base_path = f"problems/baekjoon/{dir_name}"
        
        # ë¬¸ì œëª…ì—ì„œ ë²ˆí˜¸ ë¶€ë¶„ ì œê±° (31995_ê²Œì„ë§ì˜¬ë ¤ë†“ê¸° -> ê²Œì„ë§ì˜¬ë ¤ë†“ê¸°)
        if "_" in title:
            problem_name = title.split("_", 1)[1]  # ì²« ë²ˆì§¸ ì–¸ë”ìŠ¤ì½”ì–´ ì´í›„ ë¶€ë¶„
        else:
            problem_name = title
        
        md_filename = f"{slugify(problem_name)}.md"
    
    os.makedirs(base_path, exist_ok=True)
    md_path = f"{base_path}/{md_filename}"

    if platform == "programmers":
        md_content = f"""\
# {title}

> [ë¬¸ì œ ë§í¬]({url})

## ë¬¸ì œ ì„¤ëª…
{content.get('ë¬¸ì œ ì„¤ëª…', '')}

## ì œí•œì‚¬í•­
{content.get('ì œí•œì‚¬í•­', '')}

## ì…ì¶œë ¥ ì˜ˆ
{content.get('ì…ì¶œë ¥ ì˜ˆ', '')}

## ì…ì¶œë ¥ ì˜ˆ ì„¤ëª…
{content.get('ì…ì¶œë ¥ ì˜ˆ ì„¤ëª…', '')}
"""
    else:
        md_content = f"""\
# {title}

> [ë¬¸ì œ ë§í¬]({url})

## ë¬¸ì œ
{content.get('ë¬¸ì œ', '')}

## ì…ë ¥
{content.get('ì…ë ¥', '')}

## ì¶œë ¥
{content.get('ì¶œë ¥', '')}

## ì˜ˆì œ ì…ë ¥
```
{content.get('ì˜ˆì œ ì…ë ¥', '')}
```

## ì˜ˆì œ ì¶œë ¥
```
{content.get('ì˜ˆì œ ì¶œë ¥', '')}
```
"""

    with open(md_path, "w", encoding="utf-8") as f:
        f.write(md_content)
    print(f"âœ… Markdown ìƒì„± ì™„ë£Œ: {md_path}")

def main(java_file_path):
    """ë©”ì¸ í•¨ìˆ˜"""
    print(f"ğŸš€ ì²˜ë¦¬ ì‹œì‘: {java_file_path}")
    
    # ë©”íƒ€ë°ì´í„° ì¶”ì¶œ
    title, platform, url = extract_metadata(java_file_path)
    if not all([title, platform, url]):
        print("âŒ ì£¼ì„ ì •ë³´ê°€ ë¶€ì¡±í•©ë‹ˆë‹¤.")
        print("í•„ìš”í•œ í˜•ì‹:")
        print("// NOTE : 1541_ìƒì–´ë²„ë¦°ê´„í˜¸")
        print("// NOTE : ë°±ì¤€")
        print("// NOTE : https://www.acmicpc.net/problem/1541")
        return

    platform_dir = "programmers" if "í”„ë¡œê·¸ë˜ë¨¸ìŠ¤" in platform else "baekjoon"
    print(f"ğŸ“‹ í”Œë«í¼: {platform_dir}, ì œëª©: {title}")

    if platform_dir == "programmers":
        try:
            soup = load_snapshot(url)
            
            content = extract_programmers_markdown(soup)
        except Exception as e:
            print(f"âŒ í”„ë¡œê·¸ë˜ë¨¸ìŠ¤ í˜ì´ì§€ ì ‘ê·¼ ì‹¤íŒ¨: {e}")
            return
    else:
        try:
            content = fetch_baekjoon_content(url)
        except Exception as e:
            print(f"âŒ ë°±ì¤€ ì½˜í…ì¸  ì¶”ì¶œ ì‹¤íŒ¨: {e}")
            return

    # ë§ˆí¬ë‹¤ìš´ ìƒì„±
    create_markdown(platform_dir, title, url, content)

def find_missing_markdown_files():
    """ë§ˆí¬ë‹¤ìš´ì´ ì—†ëŠ” Java íŒŒì¼ë“¤ì„ ì°¾ëŠ” í•¨ìˆ˜"""
    missing_files = []
    
    if not os.path.exists("problems"):
        print("âŒ problems í´ë”ê°€ ì¡´ì¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.")
        return missing_files
    
    # problems í´ë”ì˜ ëª¨ë“  Java íŒŒì¼ ì°¾ê¸°
    for root, dirs, files in os.walk("problems"):
        for file in files:
            if file.endswith(".java"):
                java_path = os.path.join(root, file)
                
                # ê°™ì€ ë””ë ‰í† ë¦¬ì— .md íŒŒì¼ì´ ìˆëŠ”ì§€ í™•ì¸
                md_files = [f for f in files if f.endswith(".md")]
                
                if not md_files:
                    missing_files.append(java_path)
                    print(f"ğŸ“ ë§ˆí¬ë‹¤ìš´ ëˆ„ë½: {java_path}")
                else:
                    print(f"âœ… ë§ˆí¬ë‹¤ìš´ ì¡´ì¬: {java_path}")
    
    return missing_files

if __name__ == "__main__":
    if len(sys.argv) < 2:
        print("ğŸ” ë§ˆí¬ë‹¤ìš´ì´ ì—†ëŠ” Java íŒŒì¼ë“¤ì„ ì°¾ëŠ” ì¤‘...")
        missing_files = find_missing_markdown_files()
        
        if missing_files:
            print(f"\nğŸ¯ {len(missing_files)}ê°œì˜ íŒŒì¼ì„ ì²˜ë¦¬í•©ë‹ˆë‹¤:")
            for java_file in missing_files:
                print(f"\nProcessing: {java_file}")
                main(java_file)
            print(f"\nâœ… ì´ {len(missing_files)}ê°œ íŒŒì¼ ì²˜ë¦¬ ì™„ë£Œ!")
        else:
            print("âœ… ëª¨ë“  Java íŒŒì¼ì— ë§ˆí¬ë‹¤ìš´ì´ ì¡´ì¬í•©ë‹ˆë‹¤.")
        
    else:
        # íŠ¹ì • íŒŒì¼ ì²˜ë¦¬
        for arg in sys.argv[1:]:
            if arg.endswith(".java"):
                main(arg)
