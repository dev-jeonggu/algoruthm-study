import os
import re
import sys
import requests
import certifi
from bs4 import BeautifulSoup, Tag
from datetime import datetime

def extract_metadata(java_file_path):
    with open(java_file_path, 'r', encoding='utf-8') as f:
        lines = f.readlines()

    title = platform = url = None
    for line in lines:
        if "NOTE" in line:
            content = line.strip().split(":", 1)[-1].strip()
            if content.startswith("http"):
                url = content
            elif content.lower() in ["í”„ë¡œê·¸ë˜ë¨¸ìŠ¤", "ë°±ì¤€"]:
                platform = content.lower()
            else:
                title = content.strip()

    return title, platform, url

def slugify(text):
    return text.strip().replace(" ", "_").replace(":", "").replace("/", "_")

def extract_programmers_markdown(soup):
    content = {
        "ë¬¸ì œ ì„¤ëª…": "",
        "ì œí•œì‚¬í•­": "",
        "ì…ì¶œë ¥ ì˜ˆ": "",
        "ì…ì¶œë ¥ ì˜ˆ ì„¤ëª…": ""
    }
    desc_box = soup.select_one(".markdown")
    if not desc_box:
        return content

    current = "ë¬¸ì œ ì„¤ëª…"
    buffer = []

    for tag in desc_box.children:
        if isinstance(tag, Tag):
            if tag.name == 'h5':
                if buffer:
                    content[current] = "\n".join(buffer).strip()
                    buffer = []
                heading = tag.get_text(strip=True)
                if heading in content:
                    current = heading
            elif tag.name == 'ul':
                for li in tag.find_all('li'):
                    buffer.append(f"- {li.get_text(strip=True)}")
            elif tag.name == 'table':
                rows = tag.find_all('tr')
                table_lines = []
                for i, row in enumerate(rows):
                    cols = [td.get_text(strip=True) for td in row.find_all(['td', 'th'])]
                    line = " | ".join(cols)
                    table_lines.append(line)
                    if i == 0:
                        table_lines.append(" | ".join(['---'] * len(cols)))
                buffer.extend(table_lines)
            elif tag.name == 'p':
                buffer.append(tag.get_text(strip=True))
            elif tag.name == 'br':
                buffer.append("\n")

    if buffer:
        content[current] = "\n".join(buffer).strip()

    return content

def fetch_baekjoon_content(url):
    try:
        headers = {
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'
        }
        response = requests.get(url, headers=headers, verify=certifi.where(), timeout=30)
        response.raise_for_status()
    except Exception as e:
        raise Exception(f"âŒ ë°±ì¤€ URL ì ‘ì† ì‹¤íŒ¨: {url}\n{str(e)}")

    soup = BeautifulSoup(response.text, 'html.parser')

    # í˜ì´ì§€ ì „ì²´ í…ìŠ¤íŠ¸ ì¶”ì¶œ
    page_text = soup.get_text()
    lines = [line.strip() for line in page_text.split('\n') if line.strip()]

    ë¬¸ì œ = ""
    ì…ë ¥ = ""
    ì¶œë ¥ = ""
    ì˜ˆì œì…ë ¥ = ""
    ì˜ˆì œì¶œë ¥ = ""
    
    # ì •ë‹µ ë¹„ìœ¨ ì´í›„ë¶€í„° ë¬¸ì œ ì„¤ëª… ì‹œì‘
    ë¬¸ì œ_start = -1
    ë¬¸ì œ_end = -1

    for i, line in enumerate(lines):
        # ì •ë‹µ ë¹„ìœ¨ì´ í¬í•¨ëœ ë¼ì¸ ì°¾ê¸° (ì˜ˆ: "83.805%")
        if re.search(r'\d+\.\d+%', line):
            ë¬¸ì œ_start = i + 1
        # "ì²« ë²ˆì§¸ ì¤„ì—"ê°€ ë‚˜ì˜¤ë©´ ë¬¸ì œ ì„¤ëª… ë
        elif 'ì²« ë²ˆì§¸ ì¤„ì—' in line and ë¬¸ì œ_start != -1:
            ë¬¸ì œ_end = i
            break
    # ë¬¸ì œ ì„¤ëª… ì¶”ì¶œ
    if ë¬¸ì œ_start != -1 and ë¬¸ì œ_end != -1:
        ë¬¸ì œ_lines = lines[ë¬¸ì œ_start:ë¬¸ì œ_end]
        ë¬¸ì œ = ' '.join(ë¬¸ì œ_lines).strip()

    # ì…ë ¥ ì„¤ëª… ì¶”ì¶œ ("ì²« ë²ˆì§¸ ì¤„ì—"ë¶€í„° ì‹œì‘)
    ì…ë ¥_lines = []
    ì…ë ¥_found = False
    
    for i, line in enumerate(lines):
        if 'ì²« ë²ˆì§¸ ì¤„ì—' in line:
            ì…ë ¥_found = True
            ì…ë ¥_lines.append(line)
        elif ì…ë ¥_found and ('ì¶œë ¥í•œë‹¤' in line or 'ì¶œë ¥í•˜ì‹œì˜¤' in line):
            # ì¶œë ¥ ì„¤ëª…ê¹Œì§€ í¬í•¨í•˜ì§€ ì•Šê³  ì…ë ¥ë§Œ
            break
        elif ì…ë ¥_found and 'ë²ˆì§¸ ì¤„ì—' in line:
            ì…ë ¥_lines.append(line)
    
    ì…ë ¥ = ' '.join(ì…ë ¥_lines).strip()
    
    # ì¶œë ¥ ì„¤ëª… ì¶”ì¶œ
    for line in lines:
        if ('ì¶œë ¥í•œë‹¤' in line or 'ì¶œë ¥í•˜ì‹œì˜¤' in line) and ('ê²½ìš°ì˜ ìˆ˜' in line or 'ì¶œë ¥' in line):
            ì¶œë ¥ = line.strip()
            break
    
    # ì˜ˆì œ ì…ì¶œë ¥ ì¶”ì¶œ (ìˆ«ìë¡œë§Œ êµ¬ì„±ëœ ë¼ì¸ë“¤)
    number_lines = []
    number_pattern = re.compile(r'^\d+(\s+\d+)*$')
    
    for line in lines:
        # ìˆ«ìì™€ ê³µë°±ë§Œìœ¼ë¡œ êµ¬ì„±ëœ ë¼ì¸ ì°¾ê¸°
        if number_pattern.match(line):
            number_lines.append(line)
    
    # ì²« ë²ˆì§¸ì™€ ë‘ ë²ˆì§¸ë¥¼ ì˜ˆì œ ì…ë ¥ê³¼ ì¶œë ¥ìœ¼ë¡œ ì„¤ì •
    if len(number_lines) >= 2:
        ì˜ˆì œì…ë ¥ = number_lines[0]
        ì˜ˆì œì¶œë ¥ = number_lines[1]
    
    # ë””ë²„ê¹… ì •ë³´ ì¶œë ¥
    print(f"âœ“ ì¶”ì¶œëœ ë‚´ìš©:")
    print(f"  ë¬¸ì œ: {ë¬¸ì œ[:50]}..." if len(ë¬¸ì œ) > 50 else f"  ë¬¸ì œ: {ë¬¸ì œ}")
    print(f"  ì…ë ¥: {ì…ë ¥[:50]}..." if len(ì…ë ¥) > 50 else f"  ì…ë ¥: {ì…ë ¥}")
    print(f"  ì¶œë ¥: {ì¶œë ¥}")
    print(f"  ì˜ˆì œì…ë ¥: {ì˜ˆì œì…ë ¥}")
    print(f"  ì˜ˆì œì¶œë ¥: {ì˜ˆì œì¶œë ¥}")
    
    return {
        "ë¬¸ì œ": ë¬¸ì œ,
        "ì…ë ¥": ì…ë ¥,
        "ì¶œë ¥": ì¶œë ¥,
        "ì˜ˆì œ ì…ë ¥": ì˜ˆì œì…ë ¥,
        "ì˜ˆì œ ì¶œë ¥": ì˜ˆì œì¶œë ¥
    }

def create_markdown(platform, title, url, content):
    if platform == "programmers":
        # í”„ë¡œê·¸ë˜ë¨¸ìŠ¤: problems/programmers/ë¬¸ì œëª…/ë¬¸ì œëª….md
        dir_name = slugify(title)
        base_path = f"problems/programmers/{dir_name}"
        md_filename = f"{dir_name}.md"
    else:
        # ë°±ì¤€: problems/baekjoon/ë¬¸ì œë²ˆí˜¸_ë¬¸ì œëª…/ë¬¸ì œëª….md (ë¬¸ì œë²ˆí˜¸ ì œì™¸)
        dir_name = slugify(title)  # 31995_ê²Œì„ë§ì˜¬ë ¤ë†“ê¸°
        base_path = f"problems/baekjoon/{dir_name}"
        
        # ë¬¸ì œëª…ì—ì„œ ë²ˆí˜¸ ë¶€ë¶„ ì œê±° (31995_ê²Œì„ë§ì˜¬ë ¤ë†“ê¸° -> ê²Œì„ë§ì˜¬ë ¤ë†“ê¸°)
        if "_" in title:
            problem_name = title.split("_", 1)[1]  # ì²« ë²ˆì§¸ ì–¸ë”ìŠ¤ì½”ì–´ ì´í›„ ë¶€ë¶„
        else:
            problem_name = title
        
        md_filename = f"{slugify(problem_name)}.md"
    
    os.makedirs(base_path, exist_ok=True)
    md_path = f"{base_path}/{md_filename}"

    if platform == "programmers":
        md_content = f"""\
# {title}

> [ë¬¸ì œ ë§í¬]({url})

## ë¬¸ì œ ì„¤ëª…
{content.get('ë¬¸ì œ ì„¤ëª…', '')}

## ì œí•œì‚¬í•­
{content.get('ì œí•œì‚¬í•­', '')}

## ì…ì¶œë ¥ ì˜ˆ
{content.get('ì…ì¶œë ¥ ì˜ˆ', '')}

## ì…ì¶œë ¥ ì˜ˆ ì„¤ëª…
{content.get('ì…ì¶œë ¥ ì˜ˆ ì„¤ëª…', '')}
"""
    else:
        md_content = f"""\
# {title}

> [ë¬¸ì œ ë§í¬]({url})

## ë¬¸ì œ
{content.get('ë¬¸ì œ', '')}

## ì…ë ¥
{content.get('ì…ë ¥', '')}

## ì¶œë ¥
{content.get('ì¶œë ¥', '')}

## ì˜ˆì œ ì…ë ¥
```
{content.get('ì˜ˆì œ ì…ë ¥', '')}
```

## ì˜ˆì œ ì¶œë ¥
```
{content.get('ì˜ˆì œ ì¶œë ¥', '')}
```
"""

    with open(md_path, "w", encoding="utf-8") as f:
        f.write(md_content)
    print(f"âœ… Markdown ìƒì„± ì™„ë£Œ: {md_path}")

def main(java_file_path):
    title, platform, url = extract_metadata(java_file_path)
    if not all([title, platform, url]):
        print("âŒ ì£¼ì„ ì •ë³´ê°€ ë¶€ì¡±í•©ë‹ˆë‹¤. // NOTE : ë¬¸ì œëª…, í”Œë«í¼, URL ëª¨ë‘ í•„ìš”í•©ë‹ˆë‹¤.")
        return

    platform_dir = "programmers" if "í”„ë¡œê·¸ë˜ë¨¸ìŠ¤" in platform else "baekjoon"

    if platform_dir == "programmers":
        try:
            headers = {
                'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'
            }
            response = requests.get(url, headers=headers, timeout=30)
            response.raise_for_status()
            soup = BeautifulSoup(response.text, 'html.parser')
            content = extract_programmers_markdown(soup)
        except Exception as e:
            print(f"âŒ í”„ë¡œê·¸ë˜ë¨¸ìŠ¤ í˜ì´ì§€ ì ‘ê·¼ ì‹¤íŒ¨: {e}")
            return
    else:
        try:
            content = fetch_baekjoon_content(url)
        except Exception as e:
            print(f"âŒ ë°±ì¤€ ì½˜í…ì¸  ì¶”ì¶œ ì‹¤íŒ¨: {e}")
            return

    # ë§ˆí¬ë‹¤ìš´ ìƒì„±
    create_markdown(platform_dir, title, url, content)

def find_missing_markdown_files():
    """ë§ˆí¬ë‹¤ìš´ì´ ì—†ëŠ” Java íŒŒì¼ë“¤ì„ ì°¾ëŠ” í•¨ìˆ˜"""
    missing_files = []
    
    if not os.path.exists("problems"):
        print("âŒ problems í´ë”ê°€ ì¡´ì¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.")
        return missing_files
    
    # problems í´ë”ì˜ ëª¨ë“  Java íŒŒì¼ ì°¾ê¸°
    for root, dirs, files in os.walk("problems"):
        for file in files:
            if file.endswith(".java"):
                java_path = os.path.join(root, file)
                
                # ê°™ì€ ë””ë ‰í† ë¦¬ì— .md íŒŒì¼ì´ ìˆëŠ”ì§€ í™•ì¸
                md_files = [f for f in files if f.endswith(".md")]
                
                if not md_files:
                    missing_files.append(java_path)
                    print(f"ğŸ“ ë§ˆí¬ë‹¤ìš´ ëˆ„ë½: {java_path}")
                else:
                    print(f"âœ… ë§ˆí¬ë‹¤ìš´ ì¡´ì¬: {java_path}")
    
    return missing_files

if __name__ == "__main__":
    if len(sys.argv) < 2:
        print("ğŸ” ë§ˆí¬ë‹¤ìš´ì´ ì—†ëŠ” Java íŒŒì¼ë“¤ì„ ì°¾ëŠ” ì¤‘...")
        missing_files = find_missing_markdown_files()
        
        if missing_files:
            print(f"\nğŸ¯ {len(missing_files)}ê°œì˜ íŒŒì¼ì„ ì²˜ë¦¬í•©ë‹ˆë‹¤:")
            for java_file in missing_files:
                print(f"Processing: {java_file}")
                main(java_file)
            print(f"\nâœ… ì´ {len(missing_files)}ê°œ íŒŒì¼ ì²˜ë¦¬ ì™„ë£Œ!")
        else:
            print("âœ… ëª¨ë“  Java íŒŒì¼ì— ë§ˆí¬ë‹¤ìš´ì´ ì¡´ì¬í•©ë‹ˆë‹¤.")
        
    else:
        # íŠ¹ì • íŒŒì¼ ì²˜ë¦¬
        for arg in sys.argv[1:]:
            if arg.endswith(".java"):
                main(arg)